---
layout: post
title: "MyMicrograd: Understanding Neural Networks from First Principles"
---

I created [mymicrograd](https://github.com/ahoydave/mymicrograd), a Jupyter notebook implementation of an automatic differentiation engineâ€”a deep dive into how neural networks actually compute gradients.

The project started from curiosity about what happens under the hood in machine learning. Instead of just using TensorFlow or PyTorch as black boxes, I wanted to understand the mechanics of backpropagation and automatic differentiation at a fundamental level. Building it from scratch in a Jupyter notebook forced me to think through every step of how gradients flow backwards through a computation graph.

It's a educational project designed to demystify the core concepts of neural networks. Sometimes the best way to truly understand something is to build it yourself, mistakes and all. If you're learning machine learning and want to understand the fundamentals beyond just using libraries, check out [mymicrograd on GitHub](https://github.com/ahoydave/mymicrograd). It's a great reference for exploring how gradient descent and automatic differentiation actually work.

